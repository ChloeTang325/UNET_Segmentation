{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "call_UNET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7TFxeihpIYr"
      },
      "source": [
        "# References:\n",
        "Torchvision models: \n",
        "https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "UNET: \n",
        "https://github.com/milesial/Pytorch-UNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Uv0DI6pgJD"
      },
      "source": [
        "#Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "e_KTNmw1nAUN",
        "outputId": "18ad0a27-b989-4ffa-ba86-d7dbee7bbd0d"
      },
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "import torchvision.models as models\n",
        "import pandas as pd\n",
        "import socket\n",
        "import time\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.nn.parallel import DataParallel\n",
        "import torch.nn as nn\n",
        "import torch.torchgeometry.losses.dice\n",
        "# from NiFTIDataset import train_test_split, NiFTIDataset\n",
        "# from NiFTIDataset import NiFTIDataset\n",
        "# from utils.transforms.torchvision import Repeat, Rescale, Unsqueeze\n",
        "# from UNET_class import UNET"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b929f85c204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# from NiFTIDataset import train_test_split, NiFTIDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# from NiFTIDataset import NiFTIDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.torchgeometry'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgM-LIrWwEy6"
      },
      "source": [
        "# Call the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfT5zlCPnKvG"
      },
      "source": [
        "# MultiGPU processing\n",
        "N = torch.cuda.device_count()\n",
        "i = list(range(N))\n",
        "\n",
        "## Retrieve Dataset from Metadata Dataframe and Load with Dataloader\n",
        "# MetaData dataframe\n",
        "metadata = pd.read_csv(\"metadata/metadata.csv\")\n",
        "\n",
        "## Transforms\n",
        "# Construct the appropriate transforms needed in the neural net.\n",
        "# Normalization follows guidelines in https://pytorch.org/vision/stable/models.html.\n",
        "# Rescale the image to (0,1), then convert to 3-channel grayscale, then normalize\n",
        "# It in accordance with how it should be done using the above link.\n",
        "transform = transforms.Compose([\n",
        "    Rescale(0,1),\n",
        "    Unsqueeze(0),\n",
        "    Repeat(3,1,1),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "## Dataset and Dataloaders\n",
        "# Retrieve the dataset from info obtained in metadata dataframe\n",
        "dataset = NiFTIDataset(metadata=metadata,root='.',transform=transform,\n",
        "    slice_cols='Slice_25_Path')\n",
        "\n",
        "# Split a NiFTIDatset into two groups (training and testing) based on information specified within its metadata dataframe\n",
        "# Return a tuple containing two NiFTIDataset objects with training and testing data, respectively.\n",
        "(training_data,testing_data) = train_test_split(dataset)\n",
        "\n",
        "print('Number of data in the training dataset: ' + str(len(training_data)))\n",
        "print('Number of data in the testing dataset: ' + str(len(testing_data)) + '\\n')\n",
        "\n",
        "# load the data with dataloader\n",
        "train_dataloader = DataLoader(training_data,batch_size=32,shuffle=True)\n",
        "test_dataloader = DataLoader(testing_data,batch_size=32,shuffle=False)\n",
        "\n",
        "## Model definition and loading\n",
        "# Initialize a pre-trained VGG16 object will \n",
        "# download its weights to a cache directory.\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Select a device.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Make a directory to save information in.\n",
        "log_dir = time.strftime(\n",
        "    './runs/%b%d%y_%H-%M-%S_{}_multiclass/'.format(socket.gethostname()),\n",
        "    time.localtime()\n",
        ")\n",
        "\n",
        "# Prepare the model.\n",
        "# Freeze training for all layers\n",
        "# To save computation time and that the network would already \n",
        "# be able to extract generic features from the dataset.\n",
        "# for param in model.features.parameters():\n",
        "#     param.requires_grad = False  \n",
        "\n",
        "# https://androidkt.com/pytorch-freeze-layer-fixed-feature-extractor-transfer-learning/\n",
        "# Remove the original fully-connected layer (the last layer) and create a new one\n",
        "# Newly created modules have requires_grad=True by default\n",
        "# num_features = model.classifier[-1].in_features\n",
        "# classifier_layers = list(model.classifier.children())[:-1] # Remove the last layer\n",
        "# classifier_layers.extend([nn.Linear(in_features = num_features, out_features=4)]) # Add the new layer with outputting 2 categories\n",
        "# model.classifier = nn.Sequential(*classifier_layers) # Replace the model classifier, Overwriting the original\n",
        "\n",
        "# Make the model distributed.\n",
        "model = DataParallel(model, device_ids=i)\n",
        "\n",
        "# Use the generic prepared class to handle aspects of model training and data capture.\n",
        "unet = UNET(\n",
        "    model = model, \n",
        "    train_dataloader = train_dataloader, \n",
        "    test_dataloader = test_dataloader,\n",
        "    criterion = nn.CrossEntropyLoss(), \n",
        "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9),\n",
        "    writer = SummaryWriter(log_dir=log_dir),\n",
        "    device = device,\n",
        "    verbose = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJA9Tf7Q-1-q"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwti4cPQ-0lV"
      },
      "source": [
        "# Train the model and capture the statistics of the model at each epoch.\n",
        "stats = VGG16.model_training(numOfEpoch = 100)\n",
        "with open(log_dir + 'epoch_stats.txt', 'w') as fd:\n",
        "    fd.write(str(stats))\n",
        "\n",
        "# Save the model.\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "VGG16.save(log_dir + 'most_recent_model_dict.pt')\n",
        "\n",
        "# Release resources.\n",
        "VGG16.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}